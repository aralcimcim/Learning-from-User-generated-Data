{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cb2e6346d62ce6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2024*\n",
    "# Exercise 6: Content based Filtering\n",
    "\n",
    "In this exercise, we delve into content-based filtering, a type of recommender system that makes recommendations by utilizing the features of items and a profile of the user's preferences. Unlike collaborative filtering, which relies on the user-item interactions, content-based filtering focuses on the properties of the items themselves to make recommendations. This approach is particularly useful when we have detailed metadata about items or when dealing with cold start problems related to new users or items.\n",
    "\n",
    "An example of item content properties could be (for a music track): artist name, track title, year of release, genre, as well as the audio track itself (remember, in the collaborative filtering scenario we only worked with item ids without caring about what they actually were). Such complex information as the audio track is usually handled in a form of (item) embeddings, high-dimensional vector representations that capture the characteristics of each item. By analyzing these embeddings, we can identify items that are similar to those a user has liked in the past and recommend them accordingly.\n",
    "\n",
    "Please consult the lecture slides on content-based filtering for a recap.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD24_ex06_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD24_ex06_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, you will implement two content-based filtering algorithms using item embeddings. We provide the embeddings for each item, and your task is to find items most similar to a user's consumption history. You will then evaluate the performance of your algorithms using the normalized Discounted Cumulative Gain (nDCG) metric across different user groups.\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eabaff7ba3409e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.712746Z",
     "start_time": "2024-04-24T20:30:40.708137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity\n",
    "from rec import inter_matr_implicit\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb2c50381214b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Overview and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a182e348bc73c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This exercise utilizes a dataset that consists of user-item interactions and item embeddings. The dataset is split into several files:\n",
    "\n",
    "`*.user`: Contains information about users.\n",
    "`*.item`: Contains information about items.\n",
    "`*.inter_train`: Contains user-item interactions used for training the recommender system.\n",
    "`*.inter_test`: Contains user-item interactions held out for testing the recommender system.\n",
    "`*.id_musicnn`: Contains embeddings for each item.\n",
    "\n",
    "Let's start by loading these files and taking a closer look at their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68f81ac85fe199a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.801940Z",
     "start_time": "2024-04-24T20:30:40.765755Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users Data Head:\n",
      "   user_id country  age_at_registration gender    registration_date\n",
      "0        0      RU                   25      m  2006-06-12 13:25:12\n",
      "1        1      US                   23      m  2005-08-18 15:25:41\n",
      "2        2      FR                   25      m  2006-02-26 22:39:03\n",
      "3        3      DE                    2      m  2007-02-28 10:12:13\n",
      "4        4      UA                   23      n  2007-10-09 15:21:20\n",
      "\n",
      "Item Data Head:\n",
      "                artist                                   track  item_id\n",
      "0           Black Flag                              Rise Above        0\n",
      "1                 Blur  For Tomorrow - 2012 Remastered Version        1\n",
      "2          Damien Rice                            Moody Mooday        2\n",
      "3                 Muse                            Feeling Good        3\n",
      "4  My Bloody Valentine                                    Soon        4\n",
      "\n",
      "Training Interactions Head:\n",
      "   user_id  item_id  listening_events\n",
      "0      510       50                 3\n",
      "1      510      324                 5\n",
      "2      510       80                 4\n",
      "3      510      266                 3\n",
      "4      510      152                 2\n",
      "\n",
      "Testing Interactions Head:\n",
      "   user_id  item_id  listening_events\n",
      "0      510        0                 6\n",
      "1      510       23                 2\n",
      "2      699       54                22\n",
      "3      699       55                 4\n",
      "4      699       43                 3\n",
      "\n",
      "Embeddings Head:\n",
      "   item_id         0         1         2         3         4         5  \\\n",
      "0      179  0.125290  0.012606  0.081399  0.080409  0.017942  0.169751   \n",
      "1       34  0.320514  0.007550  0.164825  0.008963  0.032615  0.110442   \n",
      "2      107  0.016870  0.000331  0.005580  0.666278  0.000631  0.223498   \n",
      "3       51  0.095227  0.000541  0.010128  0.213568  0.003663  0.117152   \n",
      "4      158  0.305471  0.001308  0.021437  0.003959  0.003706  0.075450   \n",
      "\n",
      "          6         7         8  ...        40        41        42        43  \\\n",
      "0  0.078891  0.019038  0.018354  ...  0.106704  0.038682  0.002625  0.001995   \n",
      "1  0.017253  0.171200  0.012153  ...  0.028441  0.005372  0.000978  0.004036   \n",
      "2  0.291173  0.280088  0.379135  ...  0.004347  0.063467  0.000037  0.000170   \n",
      "3  0.139817  0.246973  0.170498  ...  0.121255  0.024034  0.000129  0.000223   \n",
      "4  0.004312  0.773173  0.099721  ...  0.009150  0.000868  0.000152  0.000584   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  0.007829  0.024301  0.015865  0.001093  0.092035  0.003197  \n",
      "1  0.006219  0.009023  0.089043  0.004408  0.022120  0.001234  \n",
      "2  0.021398  0.064615  0.000129  0.047967  0.005985  0.000318  \n",
      "3  0.007875  0.020640  0.010570  0.010205  0.091741  0.000644  \n",
      "4  0.003667  0.002294  0.041627  0.170266  0.005763  0.000170  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "# Load User Data\n",
    "users = read('lfm-tiny-tunes', 'user')\n",
    "print(\"Users Data Head:\")\n",
    "print(users.head())\n",
    "\n",
    "# Load Item Data\n",
    "items = read('lfm-tiny-tunes', 'item')\n",
    "print(\"\\nItem Data Head:\")\n",
    "print(items.head())\n",
    "\n",
    "# Load Training Interactions\n",
    "train_inters = read('lfm-tiny-tunes', 'inter_train')\n",
    "print(\"\\nTraining Interactions Head:\")\n",
    "print(train_inters.head())\n",
    "\n",
    "# Load Testing Interactions\n",
    "test_inters = read('lfm-tiny-tunes', 'inter_test')\n",
    "print(\"\\nTesting Interactions Head:\")\n",
    "print(test_inters.head())\n",
    "\n",
    "# Load Embeddings\n",
    "embedding = read('lfm-tiny-tunes', 'id_musicnn')\n",
    "print(\"\\nEmbeddings Head:\")\n",
    "print(embedding.head())\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users, items, train_inters, 'lfm-tiny-tunes')\n",
    "test_interaction_matrix = inter_matr_implicit(users, items, test_inters, 'lfm-tiny-tunes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d77c5201de7d0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Item Similarity Calculation\n",
    "\n",
    "Here you can experiment with the cosine similarity between two item embeddings, to get a feeling what sensible inputs and outputs are. Cosine similarity is a measure that calculates the cosine of the angle between two vectors, often used to measure item similarity in recommendation systems. We will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e4cdcac7078c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.809301Z",
     "start_time": "2024-04-24T20:30:40.803963Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.10540926]]\n"
     ]
    }
   ],
   "source": [
    "embedding_x = np.array([0.1, 0.2, 0.3, 0.4]).reshape(1, -1)\n",
    "embedding_y = np.array([0.0, 0.1, 0.1, -0.1]).reshape(1, -1)\n",
    "similarity = cosine_similarity(embedding_x, embedding_y)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n",
    "assert -1 <= similarity <= 1, \"Cosine similarity is out of bounds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab200e04304908",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='red'>TASK 1/3</font>: Implementing the Average Embedding Similarity Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ef00b8765c030",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The idea of the first content-based recommender is to use the embeddings of the items the user already consumed to create one representation of the user's taste (by just averaging them). Then the recommedation score is assigned to each of the items in the collection as cosine_similarity between the user's profile and each item's embedding. Highest scoring items not present in the user's history are then selected for recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63174e832805d24e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, familiarize yourself with the concept of item embeddings, which are vector representations capturing the essential features or qualities of each item.\n",
    "\n",
    "After that, develop a function that takes a user's interaction history (items they have already interacted with/seen) and the item embeddings as input. This function should:\n",
    "\n",
    "* Create the user's profile embedding as discussed, calculate the similarity between the profile and all items in the collection.\n",
    "* Rank the items based on their similarity scores.\n",
    "* To ensure that only unseen items are recommended, set the aggregated similarity scores of the items the user has already interacted with to **0** as a floating point number. This will effectively remove them from consideration during the ranking process.\n",
    "* Recommend the top-K most similar items that the user has not interacted with yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac3d92646de6fa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.827Z",
     "start_time": "2024-04-24T20:30:40.820308Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_user_profile_embedding(seen_item_ids: list, item_embeddings: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the average embedding of items a user has interacted with to create a user profile embedding.\n",
    "\n",
    "    Parameters:\n",
    "    - seen_item_ids - list[int], IDs of items already seen by the user, used to filter the embeddings;\n",
    "    - item_embeddings - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: 1D numpy array, representing the user's average embedding profile.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_profile_embedding = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    seen_embeddings = item_embeddings[item_embeddings['item_id'].isin(seen_item_ids)]\n",
    "    user_profile_embedding = seen_embeddings.drop(columns='item_id').sum(axis=0)\n",
    "\n",
    "    return user_profile_embedding\n",
    "\n",
    "\n",
    "def average_embedding_similarity_rec(seen_item_ids: list, item_embeddings: pd.DataFrame, _calculate_user_profile_embedding: Callable[[List[int], pd.DataFrame], np.ndarray], top_k: int=10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recommends items to a user based on the average embedding similarity of items they have interacted with.\n",
    "    It computes the cosine similarity between the user profile and all other items, recommending\n",
    "    the top-K most similar items that the user has not yet interacted with.\n",
    "\n",
    "    seen_item_ids - list[int], ids of items already seen by the user (to exclude from recommendation);\n",
    "    embedding - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "    _calculate_user_profile_embedding - function, function to calculate the user's average embedding profile;\n",
    "    topK - int, number of recommendations per user to be returned;\n",
    "\n",
    "    returns - 1D np.ndarray, array of IDs of the top-K recommended items, sorted by decreasing similarity\n",
    "            to the user's average embedding profile;\n",
    "    \"\"\"\n",
    "\n",
    "    recommended_item_ids = None\n",
    "    user_profile_embedding = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    user_profile_embedding = _calculate_user_profile_embedding(seen_item_ids, item_embeddings)\n",
    "    user_profile_embedding_reshaped = user_profile_embedding.values.reshape(1, -1)\n",
    "    embed_values = item_embeddings.drop(columns='item_id').values\n",
    "    similarities = cosine_similarity(user_profile_embedding_reshaped, embed_values)\n",
    "    similarities_df = pd.DataFrame(similarities, columns=item_embeddings['item_id'])\n",
    "    similarities_df.loc[:, seen_item_ids] = 0\n",
    "    \n",
    "    recommended_item_ids = similarities_df.T.nlargest(top_k, 0).index.values\n",
    "\n",
    "    return recommended_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9d389aa0ff4dec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.843202Z",
     "start_time": "2024-04-24T20:30:40.835007Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items for User 1: [244  53 259 201 293]\n"
     ]
    }
   ],
   "source": [
    "user_id_example = users['user_id'].iloc[1]\n",
    "seen_item_ids = train_inters[train_inters['user_id'] == user_id_example]['item_id'].values.tolist()\n",
    "recommended_items = average_embedding_similarity_rec(seen_item_ids, embedding, calculate_user_profile_embedding, top_k=5)\n",
    "print(f\"Recommended Items for User {user_id_example}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9cad1eac118d3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='red'>TASK 2/3</font>: Implementing the Aggregated Item Similarity Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538228199764661",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this task, you would be required to implement what we call Aggregated Item Similarity Recommender. This technique also builds upon the concept of item embeddings and similarity calculations, but instead of creating a single user profile, it considers the individual similarities between each item the user has interacted with and all other items. This allows for a more diverse set of recommendations that capture different aspects of the user's preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7d384018c47a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* For each item the user has interacted with, calculate its similarity to all other items in the dataset using their embeddings (using cosine similarity). Make sure to sort the item embeddings before calling `_aggregated_item_similarity_rec`.\n",
    "* For each item that could potentially be recommended, combine the similarity scores it received from each of the items the user has already interacted with. This combined score will reflect the overall similarity of the potential recommendation to the user's preferences as expressed through their past interactions. Keep track of the highest similarity score encountered for each potential recommendation.\n",
    "* To ensure that only unseen items are recommended, set the aggregated similarity scores of the items the user has already interacted with to **0** as a floating point number. This will effectively remove them from consideration during the ranking process.\n",
    "* Rank the items based on their aggregated similarity scores. Recommend the top-K items that the user has not interacted with yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ab47e86c660b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.856592Z",
     "start_time": "2024-04-24T20:30:40.849212Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_aggregated_scores(seen_item_ids: list, item_embeddings: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes aggregated similarity scores for all items in the embedding DataFrame, comparing them against items the user has seen.\n",
    "    \n",
    "    seen_item_ids - list[int], ids of items already seen by the user (to exclude from recommendation);\n",
    "    embedding - pd.DataFrame, Sorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "    \n",
    "    returns - np.ndarray, array of aggregated similarity scores for all items, with higher scores indicating higher similarity;\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    seen_embeddings = item_embeddings[item_embeddings['item_id'].isin(seen_item_ids)]\n",
    "    similarity_scores = cosine_similarity(seen_embeddings.drop(columns='item_id'), item_embeddings.drop(columns='item_id'))\n",
    "    scores_agg = similarity_scores.sum(axis=0)\n",
    "    recommendation_scores = pd.DataFrame(scores_agg, index=item_embeddings['item_id'], columns=['score'])\n",
    "\n",
    "    return recommendation_scores\n",
    "\n",
    "\n",
    "def aggregated_item_similarity_rec(seen_item_ids: list, item_embeddings: pd.DataFrame, _compute_aggregated_scores: Callable[[List[int], pd.DataFrame], np.ndarray], top_k: int=10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recommends items to a user based on the items they have already seen, by sorting the calculated similarity scores\n",
    "    and selecting the top-k items.\n",
    "\n",
    "    seen_item_ids - list[int], ids of items already seen by the user (to exclude from recommendation);\n",
    "    embedding - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "    _compute_aggregated_scores - function, function to compute aggregated similarity scores for all items;\n",
    "    topK - int, number of recommendations per user to be returned;\n",
    "\n",
    "    returns - 1D np.ndarray, array of IDs of the top-K recommended items, sorted by decreasing similarity\n",
    "            to the user's average embedding profile;\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    recommendation_scores = _compute_aggregated_scores(seen_item_ids, item_embeddings)\n",
    "    recommendation_scores.loc[seen_item_ids, 'score'] = 0.0\n",
    "    recommended_item_ids = recommendation_scores['score'].nlargest(top_k).index.values\n",
    "\n",
    "    return recommended_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff89808ba836e44b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.917909Z",
     "start_time": "2024-04-24T20:30:40.891602Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items for User 1: [164 244 259 201  85]\n"
     ]
    }
   ],
   "source": [
    "user_id_example = users['user_id'].iloc[1]\n",
    "seen_items_user_example = np.where(train_interaction_matrix[user_id_example, :] > 0)[0]\n",
    "recommended_items = aggregated_item_similarity_rec(seen_items_user_example.tolist(), embedding, compute_aggregated_scores, top_k=5)\n",
    "print(f\"Recommended Items for User {user_id_example}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaad68a03922f0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='red'>TASK 3/3</font>: Evaluating Recommendations with nDCG\n",
    "\n",
    "In this task, you will evaluate the performance of the content-based filtering algorithm you've implemented, alongside other recommender systems that utilize collaborative filtering. Specifically, you will use the normalized Discounted Cumulative Gain (nDCG) metric to assess how effective each recommender system is across different user groups based on their interaction levels.\n",
    "\n",
    "You will compare the following recommender systems:\n",
    "\n",
    "Average Embedding Similarity Recommender (Avg_Item_Embd)\n",
    "Aggregated Item Similarity Recommender (Aggr_Item_Sim)\n",
    "Singular Value Decomposition (SVD)\n",
    "Item K-Nearest Neighbors (ItemKNN)\n",
    "Top Popular (TopPop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7eb8cf1c9116f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.923006Z",
     "start_time": "2024-04-24T20:30:40.918920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rec import svd_decompose, svd_recommend_to_list\n",
    "from rec import recTopK\n",
    "from rec import recTopKPop\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1f90df310e8f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.935704Z",
     "start_time": "2024-04-24T20:30:40.924014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_ndcg_by_user_groups(user_groups: dict, recommenders: dict, train_interaction_matrix: np.ndarray, test_interaction_matrix: np.ndarray,\n",
    "                                 U: np.ndarray, V: np.ndarray, item_embeddings: pd.DataFrame, _calculate_user_profile_embedding, \n",
    "                                 _compute_aggregated_scores, topK: int=10, n_neighbors: int=5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates recommender systems across user groups, calculating average nDCG scores.\n",
    "\n",
    "    user_groups - dict, keys - names of the user groups (str), values - lists of user IDs belonging to each group;\n",
    "    recommenders - dict, keys - names of recommenders (str), values - recommender functions;\n",
    "    train_interaction_matrix - 2D np.ndarray (users x items), interaction matrix from the training set;\n",
    "    test_interaction_matrix - 2D np.ndarray (users x items), interaction matrix from the test set;\n",
    "    U, V - 2D np.ndarray, matrices resulting from SVD decomposition of the interaction matrix;\n",
    "    item_embeddings - pd.DataFrame, DataFrame containing item IDs and their embeddings;\n",
    "    topK - int, number of top recommendations to consider for evaluation;\n",
    "    n_neighbors - int, number of neighbors for ItemKNN recommender;\n",
    "\n",
    "    returns - pd.DataFrame, with columns: 'User Group', 'Recommender', 'Average nDCG', containing evaluation results;\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for group_name, users in user_groups.items():\n",
    "        for recommender_name, recommender_func in tqdm(recommenders.items(), desc=f'Evaluating {group_name} Users'):\n",
    "            nDCG_scores = []\n",
    "            for user_id in users:\n",
    "                seen_items = np.where(train_interaction_matrix[user_id, :] > 0)[0]  # Items already interacted with by the user\n",
    "\n",
    "                if recommender_name == 'SVD':\n",
    "                    recommendations = recommender_func(user_id, seen_items.tolist(), U, V, topK)\n",
    "                elif recommender_name == 'ItemKNN':\n",
    "                    recommendations = recommender_func(train_interaction_matrix, user_id, topK, n_neighbors)\n",
    "                elif recommender_name == 'TopPop':\n",
    "                    recommendations = recommender_func(train_interaction_matrix, user_id, topK)\n",
    "                elif recommender_name == 'Avg_Item_Embd':\n",
    "                    recommendations = recommender_func(seen_items.tolist(), item_embeddings, _calculate_user_profile_embedding, topK)\n",
    "                elif recommender_name == 'Aggr_Item_Sim':\n",
    "                    recommendations = recommender_func(seen_items.tolist(), item_embeddings, _compute_aggregated_scores, topK)\n",
    "                else:\n",
    "                    raise NotImplementedError(f'Recommender {recommender_name} not implemented.')\n",
    "\n",
    "                if not isinstance(recommendations, np.ndarray):\n",
    "                    recommendations = np.array(recommendations)\n",
    "\n",
    "                # Calculate nDCG\n",
    "                true_relevance = test_interaction_matrix[user_id, :].reshape(1, -1)\n",
    "                predicted_scores = np.zeros((1, train_interaction_matrix.shape[1]))\n",
    "                predicted_scores[0, recommendations] = 1\n",
    "                nDCG_score = ndcg_score(true_relevance, predicted_scores)\n",
    "                nDCG_scores.append(nDCG_score)\n",
    "\n",
    "            avg_nDCG = np.mean(nDCG_scores)\n",
    "            results.append({'User Group': group_name, 'Recommender': recommender_name, 'Average nDCG': avg_nDCG})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bba35af84afd3e",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23b21f024e288b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, you will implement a function that evaluates the performance of the recommenders across different user groups based on their interaction levels. You will need to split the users into two groups: one with low interaction levels (below or equal a certain threshold) and one with high interaction levels (above the threshold). The function should then call the `evaluate_ndcg_by_user_groups` function to calculate the average nDCG scores for each recommender across the user groups. Make sure to only use the passed variables and parameters in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a60a6b6c1433ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:40.944067Z",
     "start_time": "2024-04-24T20:30:40.937712Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_recommenders(user_info: pd.DataFrame, parameters: dict, recommender: dict, user_threshold: int) -> (pd.DataFrame, dict):\n",
    "    \"\"\"\n",
    "    Evaluates recommenders across user groups based on interaction levels.\n",
    "\n",
    "    Splits users into low and high interaction groups based on a threshold and calculates\n",
    "    average nDCG scores for each recommender within each group.\n",
    "\n",
    "    user_info - pd.DataFrame, DataFrame containing user information.\n",
    "    parameters - dict, Dictionary containing data and parameters for evaluation, including:\n",
    "        train_interaction_matrix - 2D np.ndarray, test_interaction_matrix - 2D np.ndarray,\n",
    "        U - 2D np.ndarray, V - 2D np.ndarray, item_embeddings - pd.DataFrame, topK - int,\n",
    "        n_neighbors - int.\n",
    "    recommender - dict, Dictionary of recommender functions, with keys as recommender names\n",
    "                        and values as the corresponding functions.\n",
    "    user_threshold - int, Threshold for dividing users into low and high interaction groups.\n",
    "\n",
    "    returns - tuple:\n",
    "        pd.DataFrame, DataFrame containing evaluation results with columns: 'User Group',\n",
    "            'Recommender', 'Average nDCG'.\n",
    "        dict, Dictionary containing the user groups with keys 'Low Interaction' and\n",
    "            'High Interaction', and values as lists of user IDs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation_results_df = None\n",
    "\n",
    "    user_info['interactions'] = np.sum(parameters['train_interaction_matrix'] > 0, axis=1)\n",
    "\n",
    "    user_groups = {\n",
    "        'Low Interaction': user_info[user_info['interactions'] <= user_threshold].index.to_list(),\n",
    "        'High Interaction': user_info[user_info['interactions'] > user_threshold]['user_id'].index.to_list()\n",
    "    }\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    evaluation_results_df = evaluate_ndcg_by_user_groups(user_groups, \n",
    "                                                         recommender,\n",
    "                                                         parameters['train_interaction_matrix'], \n",
    "                                                         parameters['test_interaction_matrix'], \n",
    "                                                         parameters['U'], \n",
    "                                                         parameters['V'], \n",
    "                                                         parameters['item_embeddings'], \n",
    "                                                         calculate_user_profile_embedding, \n",
    "                                                         compute_aggregated_scores, \n",
    "                                                         parameters['topK'], \n",
    "                                                         parameters['n_neighbors'])\n",
    "\n",
    "    return evaluation_results_df, user_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92155767e987e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following Cell will evaluate the implemented recommenders on the given dataset. The evaluation results will be displayed in a DataFrame, showing the average nDCG scores for each recommender across different user groups. This Cell is for you to see how the input looks like. For a correct evaluation, the code below needs to run without errors and the nDCG scores need to be output as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea80c6459fd855b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:33:42.031308Z",
     "start_time": "2024-04-24T20:30:40.951099Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Low Interaction Users: 100%|██████████| 5/5 [00:43<00:00,  8.68s/it]\n",
      "Evaluating High Interaction Users: 100%|██████████| 5/5 [01:30<00:00, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users with low interaction levels: 560\n",
      "Number of Users with high interaction levels: 655\n",
      "         User Group    Recommender  Average nDCG\n",
      "0   Low Interaction  Avg_Item_Embd      0.174140\n",
      "1   Low Interaction  Aggr_Item_Sim      0.173580\n",
      "2   Low Interaction            SVD      0.223840\n",
      "3   Low Interaction        ItemKNN      0.252406\n",
      "4   Low Interaction         TopPop      0.199557\n",
      "5  High Interaction  Avg_Item_Embd      0.219581\n",
      "6  High Interaction  Aggr_Item_Sim      0.220410\n",
      "7  High Interaction            SVD      0.278832\n",
      "8  High Interaction        ItemKNN      0.326684\n",
      "9  High Interaction         TopPop      0.266074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define recommenders with correct parameters\n",
    "recommenders = {\n",
    "    'Avg_Item_Embd': average_embedding_similarity_rec,\n",
    "    'Aggr_Item_Sim': aggregated_item_similarity_rec,\n",
    "    'SVD': svd_recommend_to_list,\n",
    "    'ItemKNN': recTopK,\n",
    "    'TopPop': recTopKPop\n",
    "}\n",
    "\n",
    "U, V = svd_decompose(train_interaction_matrix)\n",
    "\n",
    "data = {\n",
    "    'train_interaction_matrix': train_interaction_matrix,\n",
    "    'test_interaction_matrix': test_interaction_matrix,\n",
    "    'U': U,\n",
    "    'V': V,\n",
    "    'item_embeddings': embedding,\n",
    "    '_calculate_user_profile_embedding': calculate_user_profile_embedding,\n",
    "    '_compute_aggregated_scores': compute_aggregated_scores,\n",
    "    'topK': 10,\n",
    "    'n_neighbors': 5}\n",
    "\n",
    "evaluation_results_df, user_groups = evaluate_recommenders(users, data, recommenders, user_threshold=5)\n",
    "print(f\"Number of Users with low interaction levels: {len(user_groups['Low Interaction'])}\")\n",
    "print(f\"Number of Users with high interaction levels: {len(user_groups['High Interaction'])}\")\n",
    "print(evaluation_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f0ffa9e56e9a7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:33:42.035935Z",
     "start_time": "2024-04-24T20:33:42.032306Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
