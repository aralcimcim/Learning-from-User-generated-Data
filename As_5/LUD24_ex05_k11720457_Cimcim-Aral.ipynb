{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2024*\n",
    "# Exercise 5: Popularity bias evaluation\n",
    "\n",
    "In this exercise we evaluate popularity bias of three different RecSys we already implemented. Note that here we utilize only one possible approach to evaluation of popularity bias from the user perspective. Refer to the slides and the recording for more details.\n",
    "\n",
    "This time recommendations for each algorithm are already provided in three separate files (can be found on the main page).\n",
    "\n",
    "Evaluating popularity bias through the mismatch (miscalibration) between popularity distributions over users' listening histories and their respective recommendations requires knowing how popular each item of the dataset is. Please use the new version of the lfm-tiny-tunes dataset where popularity categories (`popularity_bins`) from `0` (LowPop) to `2` (HighPop) have been already assigned. You will not need the test set for this exercise.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD24_ex06_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD24_ex06_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, as before, you are required to write a number of functions. Only implemented functions are graded. Insert your implementations into the templates provided. Please don't change the templates even if they are not pretty. Don't forget to test your implementation for correctness and efficiency. **Make sure to try your implementations on toy examples and sanity checks.**\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T15:41:38.759547200Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rec import inter_matr_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Implement Kullback–Leibler divergence (KL) and Jensen–Shannon divergence (JSD).\n",
    "\n",
    "#### in KL\n",
    "Make sure to\n",
    "* use the logarithm with base 2\n",
    "* add `eps` to both `P` and `Q` distributions\n",
    "* **normalize** (convert to probability) the distributions again after you have added `eps`\n",
    "\n",
    "#### for JSD\n",
    "* feel free to call your implementation of KL from it\n",
    "* refer to the recording and slides for a recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def KL(P: np.ndarray, Q: np.ndarray, eps: float = 1e-14):\n",
    "    '''\n",
    "    P - 1D np.ndarray, probability distribution;\n",
    "    Q - 1D np.ndarray, probability distribution;\n",
    "    eps - float, margin added to both distributions to avoid division by zero errors;\n",
    "\n",
    "    returns - float, divergence of distributions P,Q;\n",
    "    '''\n",
    "\n",
    "    kl = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    kl = np.sum(P * np.log2((P + eps) / (Q + eps))).item()\n",
    "\n",
    "    return kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check if you handle zeros correctly (use \"eps\" parameter)\n",
    "P = np.array([0.3, 0.6, 0.1])\n",
    "Q = np.array([0.4, 0.6, 0.0])\n",
    "\n",
    "assert type(KL(P, Q)) == float\n",
    "assert np.isclose(KL(P, Q), 4.193995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def JSD(P: np.ndarray, Q: np.ndarray, eps: float = 1e-14):\n",
    "    '''\n",
    "    P - 1D np.ndarray, probability distribution;\n",
    "    Q - 1D np.ndarray, probability distribution;\n",
    "    eps - float, margin added to both distributions to avoid division by zero errors, parameter for KL;\n",
    "\n",
    "    returns - float, divergence of distributions P,Q;\n",
    "    '''\n",
    "\n",
    "    jsd = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    M = 0.5 * (P + Q)\n",
    "    jsd = 0.5 * KL(P, M, eps) + 0.5 * KL(Q, M, eps)\n",
    "\n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check if you handle zeros correctly (use \"eps\" parameter)\n",
    "P = np.array([0.3, 0.6, 0.1])\n",
    "Q = np.array([0.4, 0.6, 0.0])\n",
    "\n",
    "assert type(JSD(P, Q)) == float\n",
    "assert np.isclose(JSD(P, Q), 0.055170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# KL divergence compares a target distribution to a reference distribution and is NOT symmetric\n",
    "# JS divergence is symmetric -  you can exchange P and Q and get the same results\n",
    "# the following asserts check this behaviour\n",
    "P = np.array([0.3, 0.6, 0.1])\n",
    "Q = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "assert KL(P, Q) != KL(Q, P)\n",
    "assert JSD(P, Q) == JSD(Q, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Evaluating algorithms for popularity bias.\n",
    "First, make sure all the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "\n",
    "items = read(\"lfm-tiny-tunes\", 'item')\n",
    "users = read(\"lfm-tiny-tunes\", 'user')\n",
    "train_inters = read(\"lfm-tiny-tunes\", 'inter_train')\n",
    "\n",
    "train_inter_matrix = inter_matr_implicit(users=users, items=items, interactions=train_inters, dataset_name=\"lfm-tiny-tunes\")\n",
    "\n",
    "# Information about item popularity: 0 - LowPop, 2 - HighPop\n",
    "item_pop_bins = items[\"popularity_bins\"].to_numpy()\n",
    "# Train set - users' listening history!\n",
    "train_interaction_matrix = train_inter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"item_pop_bins\": item_pop_bins,\n",
    "    \"user_histories\": train_interaction_matrix,\n",
    "    \"recommenders\": {}  # here you can access the recommendations for every algorithm\n",
    "}\n",
    "\n",
    "recommendations = {\"recommenders\": {\n",
    "    \"SVD\": {\n",
    "        \"recommendations\": np.array([])\n",
    "    },\n",
    "    \"ItemKNN\": {\n",
    "        \"recommendations\": np.array([])\n",
    "    },\n",
    "    \"TopPop\": {\n",
    "        \"recommendations\": np.array([])\n",
    "    },\n",
    "}}\n",
    "\n",
    "# Loading predictions\n",
    "for recommender in recommendations[\"recommenders\"].keys():\n",
    "    recommendations[\"recommenders\"][recommender][\"recommendations\"] = np.load(f\"recommendations/{recommender}.npy\")\n",
    "\n",
    "config.update(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `evaluate_jsd` and then call it from `evaluate_algorithms`. Do not use any global variables.\n",
    "For each user construct the two popularity distributions (from history and recommendations), normalize them, push through JSD and return the overall mean over users as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_jsd(recommendations: np.ndarray, user_histories: list, item_pop_bins: np.ndarray):\n",
    "    '''\n",
    "    recommendations - 2D np.ndarray, array containing indices of recommended items for every user;\n",
    "    user_histories - list of np.ndarray, containing the item indices of user interactions from the training set;\n",
    "    item_pop_bins - 1D np.ndarray, array that contains the popularity bin number of an item at the respective index;\n",
    "\n",
    "    returns - float, mean of jensen-shannon divergence over all users;\n",
    "    '''\n",
    "\n",
    "    mean_jsd = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    jsd_list = []\n",
    "\n",
    "    for user_history, user_recommendations in zip(user_histories, recommendations):\n",
    "        history_pop = np.bincount(item_pop_bins[user_history], minlength=3)\n",
    "        recom_pop = np.bincount(item_pop_bins[user_recommendations], minlength=3)\n",
    "\n",
    "        history_pop = history_pop / history_pop.sum()\n",
    "        recom_pop = recom_pop / recom_pop.sum()\n",
    "\n",
    "        jsd = JSD(history_pop, recom_pop)\n",
    "        jsd_list.append(jsd)\n",
    "\n",
    "    mean_jsd = np.mean(jsd_list)\n",
    "    \n",
    "    return mean_jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_histories = [np.array([5, 6]), np.array([7, 8, 9])]\n",
    "item_pop_bins = np.array([0, 0, 0, 0, 0, 0, 0, 1, 2, 2])\n",
    "recommendations = np.array([[0, 1, 8, 3, 4], [0, 1, 2, 3, 4]])\n",
    "\n",
    "jsd = evaluate_jsd(recommendations, user_histories, item_pop_bins)\n",
    "\n",
    "assert np.isclose(jsd, 0.554015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate_algorithms` takes the config as an input and evaluates popularity bias of all algorithms listed in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithms(config: dict):\n",
    "    '''\n",
    "    config - dictionary, provides the run config for the evaluation, use the already provided \"config\";\n",
    "\n",
    "    returns - dictionary, result dictionary as provided within the function;\n",
    "    '''\n",
    "    result = {\n",
    "        \"recommenders\": {# jsd per algorithm stored here\n",
    "            \"SVD\": {\n",
    "                \"jsd\": 0\n",
    "            },\n",
    "            \"ItemKNN\": {\n",
    "                \"jsd\": 0\n",
    "            },\n",
    "            \"TopPop\": {\n",
    "                \"jsd\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    user_histories = [np.where(user == 1)[0] for user in config[\"user_histories\"]]\n",
    "    for recommender in config[\"recommenders\"].keys():\n",
    "        recoms = config[\"recommenders\"][recommender][\"recommendations\"]\n",
    "        jsd = evaluate_jsd(recoms, user_histories, config[\"item_pop_bins\"])\n",
    "        result[\"recommenders\"][recommender][\"jsd\"] = jsd\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = evaluate_algorithms(config)\n",
    "assert isinstance(result[\"recommenders\"][\"SVD\"][\"jsd\"], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recommenders': {'SVD': {'jsd': 0.1818420677856273}, 'ItemKNN': {'jsd': 0.09441973776075932}, 'TopPop': {'jsd': 0.38613189427889766}}}\n"
     ]
    }
   ],
   "source": [
    "# Investigate your results - which algorithm has the lowest bias/divergence?\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
